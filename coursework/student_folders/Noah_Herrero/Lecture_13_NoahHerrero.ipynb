{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining & Machine Learning: Intro to Scikit-Learn\n",
    "\n",
    "*S. R. Taylor (2025)*\n",
    "\n",
    "This lecture and notebook are based on the \"Introduction to Scikit-Learn\" lecture of of G. Richards' \"Astrostatistics\" class at Drexel University (PHYS 440/540, https://github.com/gtrichards/PHYS_440_540), which in turn draws in large part from https://github.com/jakevdp/ESAC-stats-2014/blob/master/notebooks/03.1-Scikit-Learn-Intro.ipynb. \n",
    "\n",
    "##### Reading:\n",
    "\n",
    "- [Textbook](http://press.princeton.edu/titles/10159.html) Chapter 1: 1.5, 1.6.1, 1.6.2, 1.7.\n",
    "\n",
    "---\n",
    "\n",
    "***Exercises required for class participation are in <font color='red'>red</font>.***\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick software update:** Try the following task. In the cell below, place your cursor after `ls` and press `TAB`. If nothing happens then execute the subsequent cell that will downgrade the `jedi` package. After you do the pip installation you will need to close down and re-open your notebook. But if you see a dropdown menu of files and options appear then you don't need to do the pip installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lecture_10_NoahHerrero.ipynb           Lecture_8_NoahHerrero.ipynb\n",
      "Lecture_11_NoahHerrero.ipynb           Lecture_9_NoahHerrero.ipynb\n",
      "Lecture_12_NoahHerrero.ipynb           fig_correlations_dump.pkl\n",
      "Lecture_13_NoahHerrero.ipynb           fig_uniform_distribution.py\n",
      "Lecture_14_NoahHerrero.ipynb           hw3_data_1.npy\n",
      "Lecture_1a_NoahHerrero.ipynb           \u001b[34mhw_data\u001b[m\u001b[m\n",
      "Lecture_1b_NoahHerrero.ipynb           \u001b[34mmy_ptmcmc_chain\u001b[m\u001b[m\n",
      "Lecture_2_NoahHerrero.ipynb            plot_SDSS_SSPP.py\n",
      "Lecture_3_NoahHerrero.ipynb            \u001b[34mscripts\u001b[m\u001b[m\n",
      "Lecture_4_NoahHerrero3.ipynb           vu_astr8070_s25_hw1_NoahHerrero.ipynb\n",
      "Lecture_5_NoahHerrero.ipynb            vu_astr8070_s25_hw2_NoahHerrero1.ipynb\n",
      "Lecture_6_NoahHerrero.ipynb            vu_astr8070_s25_hw4_NoahHerrero.ipynb\n",
      "Lecture_7_NoahHerrero.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jedi==0.17.2\n",
      "  Downloading jedi-0.17.2-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting parso<0.8.0,>=0.7.0 (from jedi==0.17.2)\n",
      "  Downloading parso-0.7.1-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Downloading jedi-0.17.2-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading parso-0.7.1-py2.py3-none-any.whl (109 kB)\n",
      "Installing collected packages: parso, jedi\n",
      "  Attempting uninstall: parso\n",
      "    Found existing installation: parso 0.8.4\n",
      "    Uninstalling parso-0.8.4:\n",
      "      Successfully uninstalled parso-0.8.4\n",
      "  Attempting uninstall: jedi\n",
      "    Found existing installation: jedi 0.19.2\n",
      "    Uninstalling jedi-0.19.2:\n",
      "      Successfully uninstalled jedi-0.19.2\n",
      "Successfully installed jedi-0.17.2 parso-0.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install jedi==0.17.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Today we will cover the basics of [Scikit-Learn](http://scikit-learn.org), a popular package containing a collection of tools for machine learning written in Python. \n",
    "\n",
    "For Scikit-Learn's own introduction, see [http://scikit-learn.org/stable/tutorial/basic/tutorial.html#](http://scikit-learn.org/stable/tutorial/basic/tutorial.html#).\n",
    "\n",
    "In general, we are interested in taking a set of $n$ samples of data and then predicting properties of unknown data.\n",
    "\n",
    "In the simplest case the data will be one dimensional, but generally we will be dealing with multi-dimensional data.\n",
    "\n",
    "As we saw in `Lecture_1a`, we can break machine learning into a few distinct categories. Common to all of these is the concept of **training sets** and **test sets**, which is just like they sound: training data will be used to make predictions about the test data.  \n",
    "\n",
    "**The categories of maching learning are:** \n",
    "* [Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning), where our training set has \"labels\".  The \n",
    "[Scikit-Learn tools for supervised learning](http://scikit-learn.org/stable/supervised_learning.html#supervised-learning) can further be broken into two subcategories:\n",
    "    * [classification](https://en.wikipedia.org/wiki/Statistical_classification): where the data can be separated into two or more \"classes\" and we can use the labels from the training data to predict the labels for the test data.\n",
    "    * [regression](https://en.wikipedia.org/wiki/Regression_analysis): where instead of having training data with discrete labels, the \"truth\" is a continuous property and we are trying to predict the values of that property for the test data.\n",
    "    \n",
    "    \n",
    "* [Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning), where our training set does not have labels, yet we would like to empirically determine something about the data.  The [Scikit-Learn tools for unsupervised learning](http://scikit-learn.org/stable/unsupervised_learning.html#unsupervised-learning) include those that \n",
    "    * can help determine if the data can be represented as belonging to distinct groups ([clustering](https://en.wikipedia.org/wiki/Cluster_analysis)).\n",
    "    * can determine the distribution of the data within the parameter space ([density estimation](https://en.wikipedia.org/wiki/Density_estimation)).\n",
    "    * can better visualize the data (and hope to learn something in so doing) by projecting a high-dimensional space down to 2-3 dimensions ([dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About Scikit-Learn\n",
    "\n",
    "Let's get to grips with using Scikit-Learn for machine learning/data mining by taking a look at two very simple machine learning tasks.\n",
    "\n",
    "The first is a **classification** task: the figure shows a collection of two-dimensional data, colored according to two different class labels. We wish to determine a classification algorithm that can be used to draw a dividing boundary between the two clusters of points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "%run ./scripts/sklearn_ex1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This may seem like a trivial task, but it is a simple version of a very important concept. By drawing this separating line, we have **\"learned\"** a model that can ***generalize*** to new data. If you were to drop another point onto the plane that is unlabeled, this algorithm could now *predict* the color of the point.\n",
    "\n",
    "The next simple task we'll look at is a **regression** task. This is a simple best-fit line to a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "%run ./scripts/sklearn_ex2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This is another example of fitting a model to data, such that the model can make generalizations about new data. \n",
    "- The model has been **learned** from the training data and can be used to predict the result of test data.\n",
    "- We might be given an $x$-value, and the model would allow us to predict the $y$ value.  \n",
    "- Again, this might seem like a trivial problem, but it is a basic example of a type of operation that is fundamental to machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representation of Data in Scikit-learn\n",
    "\n",
    "In order to use Scikit-Learn, we need to understand the input and output of its algorithms.\n",
    "\n",
    "Most ML algorithms implemented in Scikit-Learn expect data to be stored in an **N-dimensional array or matrix**.  The arrays can be either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices. The size of the array is expected to be `[n_samples, n_features]`.\n",
    "\n",
    "- **n_samples**   \n",
    "  The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "  \n",
    "  \n",
    "- **n_features**  \n",
    "  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Simple Example: the Iris Dataset\n",
    "\n",
    "As an example of a simple dataset, we're going to take a look at the\n",
    "iris data stored by Scikit-Learn.\n",
    "The data consists of measurements of three different species of irises, which we picture here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display\n",
    "display(Image(filename='./figures/iris_setosa.jpg'))\n",
    "print(\"Iris Setosa\\n\")\n",
    "\n",
    "display(Image(filename='./figures/iris_versicolor.jpg'))\n",
    "print(\"Iris Versicolor\\n\")\n",
    "\n",
    "display(Image(filename='./figures/iris_virginica.jpg'))\n",
    "print(\"Iris Virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loading the Iris Data with Scikit-Learn\n",
    "\n",
    "Scikit-learn has a very straightforward set of data on these iris species.  The data consist of\n",
    "the following:\n",
    "\n",
    "- **Features/attributes in the Iris dataset:**\n",
    "  1. sepal length in cm\n",
    "  2. sepal width in cm\n",
    "  3. petal length in cm\n",
    "  4. petal width in cm\n",
    "\n",
    "Scikit-learn refers to the \"labels\" as \"targets\".  So, every time you see \"target\", just think \"label\" and it will make more sense.  \n",
    "- **The target classes are:**\n",
    "  1. Iris Setosa\n",
    "  2. Iris Versicolour\n",
    "  3. Iris Virginica\n",
    "  \n",
    "``Scikit-Learn`` embeds a copy of the iris CSV file along with a helper function to load it into numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Execute this cell\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`iris` is a dictionary, so we can look at the \"keys\" of the dictionary as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* `target_names` were defined above\n",
    "* `data` is the `[n_samples, n_features]` data array\n",
    "* `target` is the list of labels for all of the entries in `data`\n",
    "* `DESCR` is a README file with all of the information about the data set\n",
    "* `feature_names` were defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you are curious, the data come from measurements made in Quebec by botanist Dr. Edgar Anderson and first used for Sir Ronald Fisher's 1936 [classification paper](http://rcs.chemometrics.ru/Tutorials/classification/Fisher.pdf), see [https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can query the database to determine the shape of the data and find that there are 150 objects with 4 measurements each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(iris.data.shape)\n",
    "n_samples, n_features = iris.data.shape\n",
    "print(n_samples, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's look at both the first entry in `data` and the full $N\\times M$ data array. The first entry shows the values of the 4 features for the first object.\n",
    "\n",
    "Note the structure of the full data array. It is an `n_samples` array of arrays with `n_features`.  Scikit-learn is very picky about this exact format and it isn't always what you would generate naturally (particulary in the case where we have only one feature). More on this later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(iris.data[0])\n",
    "print(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also inpect the shape of the target (labels) array, which is an `n_samples`-dimensional array, print the values of those labels, and also learn how those numerical values relate to the names of the iris species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The shape of the target (labels) array is just an n_samples X 1 array\n",
    "print(iris.target.shape)\n",
    "\n",
    "# Here we see that the labels are given numerical values\n",
    "print(iris.target)\n",
    "\n",
    "# Use target_names to translate those numerical values to names\n",
    "print(iris.target_names)\n",
    "print(iris.target_names[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "i.e., $0 = $ setosa, $1 = $ versicolor, and $2 = $ virginica.\n",
    "\n",
    "This data is four dimensional, but we can visualize two of the dimensions\n",
    "at a time using a simple scatter-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_index = 0\n",
    "y_index = 1\n",
    "\n",
    "# this formatter will label the colorbar with the correct target names\n",
    "formatter = plt.FuncFormatter(lambda i, *args: iris.target_names[int(i)])\n",
    "\n",
    "plt.scatter(iris.data[:, x_index], iris.data[:, y_index], \n",
    "            c=iris.target, cmap=plt.cm.get_cmap('viridis', 3))\n",
    "\n",
    "plt.colorbar(ticks=[0, 1, 2], format=formatter)\n",
    "plt.clim(-0.5, 2.5)\n",
    "plt.xlabel(iris.feature_names[x_index])\n",
    "plt.ylabel(iris.feature_names[y_index]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Quick Exercise:\n",
    "\n",
    "<font color='red'>Change `x_index` and `y_index` in the above script and find a combination of two parameters that maximally separate the three classes.</font>\n",
    "\n",
    "This exercise is a preview of **dimensionality reduction**, which we'll see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other Available Data\n",
    "\n",
    "comes in three flavors:\n",
    "- **Packaged Data:** these small datasets are packaged with the scikit-learn installation,\n",
    "  and can be downloaded using the tools in ``sklearn.datasets.load_*``\n",
    "  \n",
    "\n",
    "- **Downloadable Data:** these larger datasets are available for download, and scikit-learn\n",
    "  includes tools which streamline this process.  These tools can be found in\n",
    "  ``sklearn.datasets.fetch_*``\n",
    "  \n",
    "  \n",
    "- **Generated Data:** there are several datasets which are generated from models based on a\n",
    "  random seed.  These are available in the ``sklearn.datasets.make_*``\n",
    "\n",
    "See [these docstrings]([documentation](http://scikit-learn.org/stable/datasets/index.html)) for all available `load_`, `fetch_`, and `make_` options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A similar interface to [datasets](http://www.astroml.org/examples/datasets/#) is available in ``astroML``. See [these docstrings](https://www.astroml.org/modules/classes.html?highlight=astroml%20datasets#module-astroML.datasets) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from astroML import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Principles of Machine Learning\n",
    "\n",
    "Now we can dive into the basic principles of machine learning, and how to utilize them via Scikit-Learn.\n",
    "\n",
    "After briefly introducing scikit-learn's ***Estimator*** object, we'll get an introduction to **supervised learning**, including ***classification*** and ***regression*** problems, and **unsupervised learning**, including ***dimensionality reduction*** and ***clustering*** problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Scikit-Learn Estimator Object\n",
    "\n",
    "Every algorithm in scikit-learn generates an `Estimator` object. \n",
    "\n",
    "All the **parameters** of an estimator can be set when it is instantiated, and have suitable default values.  \n",
    "\n",
    "Let's take a look at all of the parameters, attributes and methods in the [LinearRegression module](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate a LinearRegression estimator object, call it `model`\n",
    "model = LinearRegression()\n",
    "\n",
    "# Check an individual parameters\n",
    "#print(model.intercept_)\n",
    "\n",
    "# Check all of the parameters\n",
    "#print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With our model instantiated, we are ready to input some data.  So let's make a simple test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.arange(10)    # An array of 10 integers\n",
    "y_train = 2 * x_train + 1  # Some operation performed on that array\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'o');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we'd like to `fit` that data to determine the model parameters that we would need to `predict` the $y$ values of any new measurement of $x$.  \n",
    "\n",
    "The general code syntax for this sort of thing might be\n",
    "\n",
    "```\n",
    "lsqfit x_train y_train x_test y_pred\n",
    "```\n",
    "\n",
    "Scikit-Learn breaks this into two steps:\n",
    "1. **fitting**\n",
    "2. **predicting**\n",
    "    \n",
    "where the syntax of the fit looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# fit the model on our data\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bet you had some problems, right!?\n",
    "\n",
    "- That's because `Scikit-Learn` wants the training **features** as a multi-dimensional array. Even if there is only one dimension, like we have here. \n",
    "\n",
    "\n",
    "- In that case, it is looking for a column array instead of the row array that we gave it, i.e. instead of an array of shape $(10,)$ which is the standard, it wants an array of shape $(10,1)$.\n",
    "\n",
    "\n",
    "- Their standard syntax is to call this multi-dimensional training array, $X$, (capital $x$) and the training \"labels\" (which can be continuous) simply $y$.\n",
    "\n",
    "\n",
    "- If your $x$ is 1-D, then we have to turn it into something that looks like it is $N$-D.  All of the following yield the same result.  I personally like the last one since it is (sort of) readable.  It basically says to take an $x$ array and add another `feature` that is blank.  The `reshape` syntax looks like it used to be widely used, but has now mostly been replaced with the `np.newaxis` syntax.  \n",
    "\n",
    "\n",
    "- The effect of all three is to turn $x$ into a `n_samples` by 1-D *array* rather than an array of `n_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# The input data for sklearn must be 2D: (samples == N x features == 1)\n",
    "print(x_train)\n",
    "\n",
    "# All of these give the same result.  I'll adopt the convention from the last one.\n",
    "print(x_train.reshape(-1,1))\n",
    "print(x_train[:, np.newaxis])\n",
    "print(np.atleast_2d(x_train).T)\n",
    "print(x_train[:, None])\n",
    "\n",
    "X = x_train[:, None]\n",
    "y = y_train\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that you can produce random $X$s in the correct format with\n",
    "```\n",
    "N = 10\n",
    "X = np.random.random(size=(N, 1))\n",
    "```\n",
    "\n",
    "And you can go backwards with\n",
    "```\n",
    "X.squeeze()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Now we can fit the model on our data\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Estimated Model parameters**\n",
    "\n",
    "All the estimated parameters are attributes of the estimator object ending with an *underscore*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's look at the fit parameters, which are indicated by an underscore at the end\n",
    "print(model.coef_)\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The model found a line with a slope 2 and intercept 1, as we'd expect. Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Learning: Classification and Regression\n",
    "\n",
    "In **Supervised Learning**, we have a dataset consisting of both ***features*** and ***labels***. The task is to construct an estimator that is able to predict the label of an object given the set of features. A relatively simple example is predicting the species of iris given a set of measurements of its flower.\n",
    "\n",
    "Some more complicated examples are:\n",
    "\n",
    "- given a multicolor image of an object through a telescope, determine\n",
    "  whether that object is a star, a quasar, or a galaxy.\n",
    "  \n",
    "  \n",
    "- given a photograph of a person, identify the person in the photo.\n",
    "\n",
    "\n",
    "- given a list of movies a person has watched and their personal rating\n",
    "  of the movie, recommend a list of movies they would like\n",
    "  (So-called *recommender systems*: a famous example is the [Netflix Prize](http://en.wikipedia.org/wiki/Netflix_prize)).\n",
    "\n",
    "The commonality of these tasks is that there are one or more unknown properties of an object that must be determined from other observed quantities.\n",
    "\n",
    "Supervised learning is further broken down into two categories, \n",
    "1. **classification** (discrete labels)\n",
    "2. **regression** (continuous labels)\n",
    "\n",
    "For example, the task of determining whether an object is a star, a galaxy, or a quasar is a classification problem: the label is from three distinct categories. On the other hand, we might wish to estimate the age of an object based on such observations: this would be a regression problem, because the label (age) is a continuous quantity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Classification Example\n",
    "[K nearest neighbors (kNN)](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) is one of the simplest learning strategies: given a new, unknown observation, look up in your reference database which ones have the closest features and assign the predominant class.\n",
    "\n",
    "Let's try it out on our iris classification problem.  First see if you can do a kNN fit on the iris data using the 5 nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# create the model\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5) #Complete with number of neighbors\n",
    "\n",
    "# fit the model\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Now see if you can figure out what kind of iris has a 3cm x 5cm sepal and 4cm x 2cm petal using the predict method? Remember to be careful constructing your $X$ array.</font>  \n",
    "\n",
    "This is tricker than before. We now have multiple features, but we still need those in a `n_samples=1` by `n_feaures=4` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "measurements = ___  # List of measurements for 1 iris\n",
    "print(measurements)\n",
    "Xtest = np.array(___)\n",
    "Xtest = ___[___:___] # Convert the 4-element numpy array into a 1x4 array.\n",
    "print(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font color='red'>Once $X$ is properly formatted, go ahead and do the prediction.  \n",
    "\n",
    "Can you output the *name* rather than the index of the iris type?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# What kind of iris has 3cm x 5cm sepal and 4cm x 2cm petal?\n",
    "# call the \"predict\" method:\n",
    "result = knn.predict(___)\n",
    "print(result)\n",
    "\n",
    "print(___.target_names[___]) # Complete such that the name rather than the number is output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In some sense the whole point of classification is that there isn't necessarily going to be a cut and dry \"right\" answer. So, we might want to do probabilistic predictions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "knn.predict_proba(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could even visualize the full parameter space so that we could determine the classification by eye.  To make things simpler, we have done this in 2-D rather than 4-D.  Note that it isn't perfect, but does a really good job of classifying at least the red sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%run ./scripts/sklearn_ex3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "<font color='red'>Let's do the same, but now using [``sklearn.svm.SVC``](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) to predict what type of iris has a 3cm x 5cm sepal and 4cm x 2cm petal?</font>\n",
    "\n",
    "This is **Support Vector Machine Classification**. But you don't have to know what it is now in order to use it.\n",
    "\n",
    "*Careful -- our plot above overwrote some things. So, best to start from scratch and reload the iris data set and redefine the test input.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "Xtest = np.array([[3,5,4,2],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = ____()\n",
    "model.fit(____,____)\n",
    "result = ____.____(____)\n",
    "print(iris.target_names[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regression Example\n",
    "\n",
    "OK, that was a **classification** example. The simplest possible **regression** example is just fitting a line to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Create some simple data\n",
    "\n",
    "np.random.seed(0)\n",
    "x = np.random.random(size=20) # 20 random numbers between 0 an 1\n",
    "y = 3 * x + 2 + np.random.randn(20) # some operation performed on that array\n",
    "X = x[:,None]\n",
    "\n",
    "# Fit a Linear Regression\n",
    "modelLR = LinearRegression()\n",
    "#modelRF = \n",
    "modelLR.fit(X,y)\n",
    "\n",
    "# Plot the data and the model prediction\n",
    "x_test = np.linspace(0, 1, 100)\n",
    "X_test = x_test[:,None]\n",
    "y_predLR = modelLR.predict(X_test)\n",
    "\n",
    "plt.plot(x, y, 'o')\n",
    "plt.plot(X_test, y_predLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Modify the above code to add a line using the [`RandomForestRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) imported from `sklearn.ensemble` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning: Dimensionality Reduction and Clustering\n",
    "\n",
    "**Unsupervised Learning** addresses a different sort of problem. \n",
    "- Here the data has no labels.\n",
    "- We are interested in finding similarities between the objects in question. \n",
    "- In a sense, you can think of unsupervised learning as ***a means of discovering labels from the data itself***.\n",
    "\n",
    "Unsupervised learning comprises tasks such as ***dimensionality reduction***, ***clustering***, and\n",
    "***density estimation***. \n",
    "\n",
    "For example, in the iris data discussed above, we can use unsupervised methods to determine combinations of the measurements which best display the structure of the data. As we'll see below, such a projection of the data can be used to visualize the four-dimensional dataset in two dimensions. Some more involved unsupervised learning problems are:\n",
    "\n",
    "- given detailed observations of distant galaxies, determine which features or combinations of\n",
    "  features best summarize the information.\n",
    "  \n",
    "  \n",
    "- given a mixture of two sound sources (for example, a person talking over some music),\n",
    "  separate the two (this is called the [blind source separation](http://en.wikipedia.org/wiki/Blind_signal_separation) problem).\n",
    "  \n",
    "  \n",
    "- given a video, isolate a moving object and categorize in relation to other moving objects which have been seen.\n",
    "\n",
    "Sometimes the two may even be combined: e.g., unsupervised learning can be used to find useful\n",
    "features in heterogeneous data, and then these features can be used within a supervised\n",
    "framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality Reduction: PCA\n",
    "\n",
    "**[Principle Component Analysis (PCA)](http://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca)** is a dimension reduction technique that can find the combinations of variables that explain the most variance.\n",
    "\n",
    "Consider the iris dataset. It cannot be visualized in a single 2D plot, as it has 4 features. We are going to extract 2 combinations of sepal and petal dimensions to visualize it.\n",
    "\n",
    "The process starts in the same way as the other Scikit-Learn examples that we have done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But now we can't do a `predict` step since we don't have training data.  Instead we are going to use the `transform()` method.  But first let's take a look at the [parameters, attributes, and methods for PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced = pca.transform(X)\n",
    "print(\"Reduced dataset shape:\", X_reduced.shape)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "            c=y, cmap='viridis')\n",
    "\n",
    "print(\"Projection of the 2 components:\")\n",
    "for component in pca.components_:\n",
    "    print(\" + \".join(\"%.3f x %s\" % (value, name) for value, name in \n",
    "                     zip(component, iris.feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **This projection maximizes the separation of the classes.** \n",
    "- We can use this either just for visualization or to actually help define the classes.\n",
    "- From the separation of points in the PCA projection, we can easily see three distinct classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering: K-means\n",
    "\n",
    "**[Clustering](http://scikit-learn.org/stable/modules/clustering.html)** groups together observations that are homogeneous with respect to a given criterion, finding ''clusters'' in the data.\n",
    "\n",
    "Note that these clusters will uncover relevent hidden structure of the data only if the criterion used highlights it.\n",
    "\n",
    "Let's look at ***[`KMeans`](http://scikit-learn.org/stable/modules/clustering.html#k-means)***.  Note that here the [parameters, attributes, and methods](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) allow for *either* `predict` or `transform`.\n",
    "\n",
    "<font color='red'>Implement K-means with `n_clusters=3` and `random_state=0`, so that everyone should get the same answer for a 3 cluster solution.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k_means = KMeans(___,___) # Complete\n",
    "k_means.fit(___)\n",
    "y_pred = k_means.predict(___)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], \n",
    "            c=y_pred, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So `Kmeans` did roughly what we might have expected by eye from the application of `PCA`.  The green objects are all classified correctly, but there are some errors on the yellow-blue border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap: Scikit-learn's estimator interface\n",
    "\n",
    "Scikit-learn strives to have a uniform interface (API) across all methods, and we'll see examples of these below. Given a scikit-learn ***estimator*** object named `model`, the following methods are available:\n",
    "\n",
    "- Available in **all Estimators**\n",
    "     - `model.fit()`: fit training data. For supervised learning applications, this accepts two arguments: the data `X` and the labels `y` [e.g., `model.fit(X, y)`].  For unsupervised learning applications, this accepts only a single argument, the data `X` [e.g., `model.fit(X)`].\n",
    "  \n",
    "  \n",
    "- Available in **supervised estimators**\n",
    "    - `model.predict()`: given a trained model, predict the label of a new set of data. This method accepts one argument, the new data `X_new` [e.g., `model.predict(X_new)`], and returns the learned label for each object in the array.\n",
    "    - `model.predict_proba()`: For classification problems, some estimators also provide this method, which returns the probability that a new observation has each categorical label. In this case, the label with the highest probability is returned by `model.predict()`.\n",
    "    - `model.score()`: For classification or regression problems, most (all?) estimators implement a score method.  Scores are between 0 and 1, with a larger score indicating a better fit.\n",
    "    \n",
    "    \n",
    "- Available in **unsupervised estimators**\n",
    "    - `model.transform()`: given an unsupervised model, transform new data into the new basis. This also accepts one argument `X_new`, and returns the new representation of the data based on the unsupervised model.\n",
    "    - `model.fit_transform()`: some estimators implement this method, which more efficiently performs a fit and a transform on the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Validation\n",
    "\n",
    "An important aspect of ML is **model validation**:\n",
    "> *determining how well your model will generalize from the training data to future unlabeled data.* \n",
    "\n",
    "Let's look at an example using the ***nearest neighbor classifier***. This is a very simple classifier. It stores all training data, and for any unknown quantity, simply returns the label of the closest training point.\n",
    "\n",
    "With the iris data, it very easily returns the correct prediction for each of the input points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, y = iris.data, iris.target\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print('Element-wise check: ' + str(y==y_pred))\n",
    "print('All-at-once check: ' + str(np.all(y == y_pred))) # To check all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "See what happens if we used more than the first nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more useful way to look at the results is to view the **confusion matrix**, or the matrix showing the frequency of inputs and outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Read each element of the matrix as \"*the number of observations known to be in row-$i$ and predicted to be in column-$j$*\".\n",
    "\n",
    "For each class, all 50 training samples are correctly identified. But this **does not mean that our model is perfect!** In particular, such a model generalizes extremely poorly to new data. We can simulate this by splitting our data into a ***training set*** and a ***test set***. Scikit-learn contains some convenient routines to do this: here we will apply [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)\n",
    "print(confusion_matrix(ytest, ypred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This paints a better picture of the true performance of our classifier: apparently there is some confusion between the second and third species, which we might anticipate given what we've seen of the data above.\n",
    "\n",
    "This is why it's **extremely important** to use a train/test split when evaluating your models.  We'll go into more depth on model evaluation later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## One Last Example\n",
    "\n",
    "Sometimes you might want to apply multiple ML techniques. For example, just because you have labels in your training data doesn't mean that a regression algorithm will work very well.  So, you might want to start with a clustering or dimensional reduction algorithm first.\n",
    "\n",
    "Let's take a look at a really cool data set/example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Handwritten numbers data set\n",
    "\n",
    "Another built-in Scikit-Learn data set is the [handwritten digits data set](http://scikit-learn.org/stable/datasets/index.html#optical-recognition-of-handwritten-digits-data-set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "print(digits.images.shape)\n",
    "print(digits.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot a few of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(np.flipud(digits.images[i]), \n",
    "              cmap='binary')\n",
    "    ax.text(0.05, 0.05, str(digits.target[i]), \n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here the data is simply each pixel value within an $8\\times 8$ grid.  For analysis, we don't care about the $2$-D nature of the image, we can just treat it as an $8\\times8=64$ dimensional array. So our data have $1797$ samples in $64$ dimensions.\n",
    "\n",
    "Then each of the $1797$ ***images*** has a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(digits.data.shape)\n",
    "\n",
    "# The images themselves\n",
    "print(digits.images[0])\n",
    "\n",
    "# The data for use in our algorithms\n",
    "print(digits.data[0])\n",
    "\n",
    "# The labels\n",
    "print(digits.target)\n",
    "print(digits.target[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Unsupervised Learning: Dimensionality Reduction on Digits\n",
    "\n",
    "We'd like to visualize our points within the 64-dimensional parameter space, but it's difficult to plot points in 64 dimensions! \n",
    "\n",
    "Instead we'll use a unsupervised dimensionality reduction technique called **[`Isomap`](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap\n",
    "iso = Isomap(n_components=2)\n",
    "data_projected = iso.fit_transform(digits.data)\n",
    "data_projected.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`Isomap` has turned our $1797\\times 64$ dimensional data set into a $1797\\times 2$ dimensional data set. This is much easier to visualize! Now let's see if that visualization is at all helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot all of the data points in the two projected dimensions.  \n",
    "# Color the points by their labels.\n",
    "plt.scatter(data_projected[:,0], data_projected[:,1], \n",
    "            c=digits.target, edgecolor='none', alpha=0.5, \n",
    "            cmap=plt.cm.get_cmap('nipy_spectral', 10));\n",
    "\n",
    "# Add the color bar\n",
    "plt.colorbar(label='digit label', ticks=range(10))\n",
    "\n",
    "# Make it clear which color goes with which label\n",
    "plt.clim(-0.5, 9.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see here that the digits are fairly well-separated in the parameter space. This tells us that a supervised classification algorithm should perform fairly well. Let's give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Learning: Classification on Digits\n",
    "\n",
    "The first thing we'll want to do before attempting to classify the digits is to split the digits into a training and testing sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# taking 80% of the data as a training set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(digits.data, digits.target, \n",
    "                                                random_state=2, train_size=0.8)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's use a simple [`Logistic Regression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) which (despite its name) is a classification algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty='l2', max_iter=2000)\n",
    "clf.fit(Xtrain, ytrain)\n",
    "ypred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can **check our classification accuracy by comparing the true values of the test set to the predictions**.  We'll use one of sklearn's built-in [metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ytest, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This single number doesn't tell us ***where*** we've gone wrong: one nice way to do this is to use the ***confusion matrix*** as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(ytest, ypred))\n",
    "plt.imshow(np.log(confusion_matrix(ytest, ypred)), \n",
    "           cmap='Blues', interpolation='nearest')\n",
    "plt.ylabel('true')\n",
    "plt.xlabel('predicted');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So, that's $31$ \"$0$\"s were classified as \"$0$\", while one of them was classified as a \"$4$\", and no non-zero digits were so classified.\n",
    "\n",
    "\n",
    "- $41$ \"$1$\"s were classified correctly, but one each were classified as a \"$3$\", \"$8$\", and \"$9$\".\n",
    "\n",
    "\n",
    "- Moreover, one \"$5$\" and one \"$6$\" were classified as \"$1$\"s.\n",
    "\n",
    "\n",
    "- And so on.\n",
    "\n",
    "\n",
    "We can look at some of the outputs along with their predicted labels, making the bad labels red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 10, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(np.flipud(Xtest[i].reshape(8, 8)), cmap='binary')\n",
    "    ax.text(0.05, 0.05, str(ypred[i]), transform=ax.transAxes, \n",
    "            color='green' if (ytest[i] == ypred[i]) else 'red')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The interesting thing is that even with this simple logistic regression algorithm, some of the mislabeled cases are ones that we ourselves might get wrong!\n",
    "\n",
    "There are many ways to improve this classifier, for example using a more sophisticated model, use cross validation, etc.  We'll get to those in the coming weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Flow Chart: How to Choose your Estimator\n",
    "\n",
    "In the mean time, I remind you of this the [Scikit-Learn algorithm cheat sheet](http://scikit-learn.org/stable/tutorial/machine_learning_map/) which gives a nice summary of which algorithms to choose in various situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"http://scikit-learn.org/dev/_static/ml_map.png\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
